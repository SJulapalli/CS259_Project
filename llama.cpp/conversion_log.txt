INFO:hf-to-gguf:Loading model: hmt_model
INFO:hf-to-gguf:Model architecture: HMTModel
INFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}
INFO:hf-to-gguf:cross_attn.wk.weight,        torch.bfloat16 --> F16, shape = {2048, 4096}
INFO:hf-to-gguf:cross_attn.wq.weight,        torch.bfloat16 --> F16, shape = {2048, 4096}
INFO:hf-to-gguf:memory_cell.memory,          torch.float32 --> F32, shape = {2048, 1}
INFO:hf-to-gguf:memory_cell.mem_map.inv_linear.weight, torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:memory_cell.mem_map.linear.weight, torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:output.weight,               torch.float32 --> F16, shape = {2048, 128256}
INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}
DEBUG: Processing tensor 'rope_freqs.weight'
DEBUG: Processing tensor 'cross_attn.wk.weight'
DEBUG: Processing tensor 'cross_attn.wq.weight'
DEBUG: Processing tensor 'mem'
DEBUG: Processing tensor 'memory_cell.mem_map.inv_linear.weight'
DEBUG: Processing tensor 'memory_cell.mem_map.linear.weight'
DEBUG: Processing tensor 'memory_cell.memory'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.lm_head.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.embed_tokens.lora_embedding_A.default'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.embed_tokens.lora_embedding_B.default'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight'
INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {2048}
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.input_layernorm.weight'
INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight'
INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight'
INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.post_attention_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.input_layernorm.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.mlp.down_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.mlp.up_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.post_attention_layernorm.weight'
INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}
INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {2048}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 131072
INFO:hf-to-gguf:gguf: embedding length = 2048
INFO:hf-to-gguf:gguf: feed forward length = 8192
INFO:hf-to-gguf:gguf: head count = 32
INFO:hf-to-gguf:gguf: key-value head count = 8
INFO:hf-to-gguf:gguf: rope theta = 500000.0
INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model quantization version
INFO:hf-to-gguf:Set model tokenizer
WARNING:gguf.vocab:Unknown separator token '<|begin_of_text|>' in TemplateProcessing<pair>
INFO:gguf.vocab:Adding 280147 merge(s).
INFO:gguf.vocab:Setting special token type bos to 128000
INFO:gguf.vocab:Setting special token type eos to 128001
INFO:gguf.vocab:Setting add_bos_token to True
INFO:gguf.vocab:Setting add_sep_token to False
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:hmt_llama3.2-1b.gguf: n_tensors = 152, total_size = 2.5G
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight'
DEBUG: Processing tensor 'memory_cell.model.base_model.model.model.norm.weight'
Writing:   0%|          | 0.00/2.52G [00:00<?, ?byte/s]Writing:  23%|       | 576M/2.52G [00:01<00:06, 303Mbyte/s]Writing:  24%|       | 609M/2.52G [00:02<00:06, 292Mbyte/s]Writing:  27%|       | 676M/2.52G [00:02<00:05, 312Mbyte/s]Writing:  29%|       | 731M/2.52G [00:02<00:05, 335Mbyte/s]Writing:  32%|      | 798M/2.52G [00:02<00:04, 371Mbyte/s]Writing:  34%|      | 853M/2.52G [00:02<00:04, 350Mbyte/s]Writing:  36%|      | 920M/2.52G [00:02<00:05, 307Mbyte/s]Writing:  39%|      | 974M/2.52G [00:03<00:05, 284Mbyte/s]Writing:  40%|      | 1.01G/2.52G [00:03<00:05, 272Mbyte/s]Writing:  41%|     | 1.04G/2.52G [00:03<00:05, 260Mbyte/s]Writing:  43%|     | 1.10G/2.52G [00:03<00:05, 246Mbyte/s]Writing:  45%|     | 1.13G/2.52G [00:03<00:05, 244Mbyte/s]Writing:  46%|     | 1.17G/2.52G [00:03<00:05, 261Mbyte/s]Writing:  48%|     | 1.22G/2.52G [00:04<00:04, 301Mbyte/s]Writing:  51%|     | 1.28G/2.52G [00:04<00:03, 329Mbyte/s]Writing:  53%|    | 1.34G/2.52G [00:04<00:03, 317Mbyte/s]Writing:  54%|    | 1.37G/2.52G [00:04<00:03, 308Mbyte/s]Writing:  56%|    | 1.41G/2.52G [00:04<00:04, 269Mbyte/s]Writing:  59%|    | 1.49G/2.52G [00:04<00:02, 370Mbyte/s]Writing:  63%|   | 1.58G/2.52G [00:04<00:02, 469Mbyte/s]Writing:  65%|   | 1.65G/2.52G [00:05<00:01, 506Mbyte/s]Writing:  68%|   | 1.70G/2.52G [00:05<00:01, 499Mbyte/s]Writing:  70%|   | 1.77G/2.52G [00:05<00:01, 493Mbyte/s]Writing:  74%|  | 1.86G/2.52G [00:05<00:01, 523Mbyte/s]Writing:  76%|  | 1.91G/2.52G [00:05<00:01, 527Mbyte/s]Writing:  79%|  | 1.98G/2.52G [00:05<00:00, 545Mbyte/s]Writing:  82%| | 2.07G/2.52G [00:05<00:00, 589Mbyte/s]Writing:  85%| | 2.15G/2.52G [00:05<00:00, 620Mbyte/s]Writing:  88%| | 2.22G/2.52G [00:06<00:00, 603Mbyte/s]Writing:  92%|| 2.31G/2.52G [00:06<00:00, 567Mbyte/s]Writing:  94%|| 2.38G/2.52G [00:06<00:00, 406Mbyte/s]Writing:  97%|| 2.43G/2.52G [00:06<00:00, 337Mbyte/s]Writing:  99%|| 2.50G/2.52G [00:07<00:00, 273Mbyte/s]Writing: 100%|| 2.52G/2.52G [00:07<00:00, 348Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to hmt_llama3.2-1b.gguf

Run the project from within build-local.

make sure to run cmake and make to set up llama-cli

The way I usually run it is ./bin/llama-cli -p "<Your prompt here>" -m model.gguf -b batch_size(necessary for hmt style segmentation) -c context_length(must fit prompt) -n num_output_tokens

You can similarly quantize a model with llama-quantize.

Getting a model into .gguf format can be done with convert_hf_to_gguf.py if you have a safetensors file in HMT format. In order to properly match the expected format, you need to have your file merge
the lora layers into a single layer. This can be done by running main in HMT-pytorch with the same setup we're used to for accelerate, which will output the safetensors file. I pretty much just commented everything
that actually has to do with running/training the model out, so it'll just set up HMT and save it in safetensors format. The file structure after this is relevantly different; you can look through it by downloading safetensors_explorer if you want how it is structured once it comes out.

when doing the conversion from safetensors to gguf, you need to have a separate directory with the safetensors file and the tokenizer.json, tokenizer_config.json, special_tokens_map.json, and config.json files from the original llama 3.2 1b model. We basically inherit all of those properties since we use llama 3.2 1b as our base model.

Display this file in raw to actually see the file structure

SafeTensors Explorer - hmt_model/model.safetensors (1/1)
Use â†‘/â†“ to navigate, Enter/Space to expand/collapse, / to search, q to quit
================================================================================
â–¼ ğŸ“ ğŸ”§ Metadata (0 tensors, 0 B)
    ğŸ·ï¸  memory_cell.model.model.embed_tokens.weight [string]: memory_cell.model.lm_head.weight
â–¼ ğŸ“ cross_attn (2 tensors, 32.0 MB)
  â–¼ ğŸ“ wk (1 tensors, 16.0 MB)
      ğŸ“„ weight [BF16, (4096, 2048), 16.0 MB]
  â–¼ ğŸ“ wq (1 tensors, 16.0 MB)
      ğŸ“„ weight [BF16, (4096, 2048), 16.0 MB]
  ğŸ“„ mem [F32, (1, 2048), 8.0 KB]
â–¼ ğŸ“ memory_cell (149 tensors, 4.6 GB)
  â–¼ ğŸ“ mem_map (2 tensors, 32.0 MB)
    â–¼ ğŸ“ inv_linear (1 tensors, 16.0 MB)
        ğŸ“„ weight [F32, (2048, 2048), 16.0 MB]
    â–¼ ğŸ“ linear (1 tensors, 16.0 MB)
        ğŸ“„ weight [F32, (2048, 2048), 16.0 MB]
    ğŸ“„ memory [F32, (1, 2048), 8.0 KB]
  â–¼ ğŸ“ model (146 tensors, 4.6 GB)
    â–¼ ğŸ“ lm_head (1 tensors, 1002.0 MB)
        ğŸ“„ weight [F32, (128256, 2048), 1002.0 MB]
    â–¼ ğŸ“ model (145 tensors, 3.6 GB)
      â–¼ ğŸ“ layers (144 tensors, 3.6 GB)
        â–¼ ğŸ“ 0 (9 tensors, 232.0 MB)
          â–¼ ğŸ“ input_layernorm (1 tensors, 8.0 KB)
              ğŸ“„ weight [F32, (2048), 8.0 KB]
          â–¼ ğŸ“ mlp (3 tensors, 192.0 MB)
            â–¼ ğŸ“ down_proj (1 tensors, 64.0 MB)
                ğŸ“„ weight [F32, (2048, 8192), 64.0 MB]
            â–¼ ğŸ“ gate_proj (1 tensors, 64.0 MB)
                ğŸ“„ weight [F32, (8192, 2048), 64.0 MB]
            â–¼ ğŸ“ up_proj (1 tensors, 64.0 MB)
                ğŸ“„ weight [F32, (8192, 2048), 64.0 MB]
          â–¼ ğŸ“ post_attention_layernorm (1 tensors, 8.0 KB)
              ğŸ“„ weight [F32, (2048), 8.0 KB]
          â–¼ ğŸ“ self_attn (4 tensors, 40.0 MB)
            â–¼ ğŸ“ k_proj (1 tensors, 4.0 MB)
                ğŸ“„ weight [F32, (512, 2048), 4.0 MB]
            â–¼ ğŸ“ o_proj (1 tensors, 16.0 MB)
                ğŸ“„ weight [F32, (2048, 2048), 16.0 MB]
            â–¼ ğŸ“ q_proj (1 tensors, 16.0 MB)
                ğŸ“„ weight [F32, (2048, 2048), 16.0 MB]
            â–¼ ğŸ“ v_proj (1 tensors, 4.0 MB)
                ğŸ“„ weight [F32, (512, 2048), 4.0 MB]
        â–¶ ğŸ“ 1 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 2 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 3 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 4 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 5 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 6 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 7 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 8 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 9 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 10 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 11 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 12 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 13 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 14 (9 tensors, 232.0 MB)
        â–¶ ğŸ“ 15 (9 tensors, 232.0 MB)
      â–¼ ğŸ“ norm (1 tensors, 8.0 KB)
          ğŸ“„ weight [F32, (2048), 8.0 KB]


SafeTensors Explorer - llama.cpp/build-local/hmt-model-f32.gguf (1/1)
Use â†‘/â†“ to navigate, Enter/Space to expand/collapse, / to search, q to quit
================================================================================
    ğŸ·ï¸  tokenizer.ggml.add_sep_token [bool]: false
    ğŸ·ï¸  tokenizer.ggml.bos_token_id [u32]: 128000
    ğŸ·ï¸  tokenizer.ggml.eos_token_id [u32]: 128001
    ğŸ·ï¸  tokenizer.ggml.merges [array]: ["Ä  Ä ", "Ä  Ä Ä Ä ", ..., "Ã©Ä¶ Â¦" (280147)]
    ğŸ·ï¸  tokenizer.ggml.model [string]: "gpt2"
    ğŸ·ï¸  tokenizer.ggml.pre [string]: "llama-bpe"
    ğŸ·ï¸  tokenizer.ggml.token_type [array]: [1, 1, ..., 3 (128256)]
    ğŸ·ï¸  tokenizer.ggml.tokens [array]: ["!", """, ..., "<|reserved_special_token_247|>...
â–¼ ğŸ“ blk (144 tensors, 3.6 GB)
  â–¼ ğŸ“ 0 (9 tensors, 232.0 MB)
    â–¼ ğŸ“ attn_k (1 tensors, 4.0 MB)
        ğŸ“„ weight [F32, (2048, 512), 4.0 MB]
    â–¼ ğŸ“ attn_norm (1 tensors, 8.0 KB)
        ğŸ“„ weight [F32, (2048), 8.0 KB]
    â–¼ ğŸ“ attn_output (1 tensors, 16.0 MB)
        ğŸ“„ weight [F32, (2048, 2048), 16.0 MB]
    â–¼ ğŸ“ attn_q (1 tensors, 16.0 MB)
        ğŸ“„ weight [F32, (2048, 2048), 16.0 MB]
    â–¼ ğŸ“ attn_v (1 tensors, 4.0 MB)
        ğŸ“„ weight [F32, (2048, 512), 4.0 MB]
    â–¼ ğŸ“ ffn_down (1 tensors, 64.0 MB)
        ğŸ“„ weight [F32, (8192, 2048), 64.0 MB]
    â–¼ ğŸ“ ffn_gate (1 tensors, 64.0 MB)
        ğŸ“„ weight [F32, (2048, 8192), 64.0 MB]
    â–¼ ğŸ“ ffn_norm (1 tensors, 8.0 KB)
        ğŸ“„ weight [F32, (2048), 8.0 KB]
    â–¼ ğŸ“ ffn_up (1 tensors, 64.0 MB)
        ğŸ“„ weight [F32, (2048, 8192), 64.0 MB]
  â–¶ ğŸ“ 1 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 2 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 3 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 4 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 5 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 6 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 7 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 8 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 9 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 10 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 11 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 12 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 13 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 14 (9 tensors, 232.0 MB)
  â–¶ ğŸ“ 15 (9 tensors, 232.0 MB)
â–¼ ğŸ“ hmt (6 tensors, 96.0 MB)
  â–¼ ğŸ“ cross_attn_k (1 tensors, 32.0 MB)
      ğŸ“„ weight [F32, (2048, 4096), 32.0 MB]
  â–¼ ğŸ“ cross_attn_q (1 tensors, 32.0 MB)
      ğŸ“„ weight [F32, (2048, 4096), 32.0 MB]
  â–¼ ğŸ“ initial_memory (1 tensors, 8.0 KB)
      ğŸ“„ weight [F32, (2048), 8.0 KB]
  â–¼ ğŸ“ mem_map (2 tensors, 32.0 MB)
    â–¼ ğŸ“ inv (1 tensors, 16.0 MB)
        ğŸ“„ weight [F32, (2048, 2048), 16.0 MB]
    â–¼ ğŸ“ linear (1 tensors, 16.0 MB)
        ğŸ“„ weight [F32, (2048, 2048), 16.0 MB]
  â–¼ ğŸ“ summary_prompt (1 tensors, 8.0 KB)
      ğŸ“„ weight [F32, (2048), 8.0 KB]
â–¼ ğŸ“ output (1 tensors, 1002.0 MB)
    ğŸ“„ weight [F32, (2048, 128256), 1002.0 MB]
â–¼ ğŸ“ output_norm (1 tensors, 8.0 KB)
    ğŸ“„ weight [F32, (2048), 8.0 KB]
â–¼ ğŸ“ token_embd (1 tensors, 1002.0 MB)
    ğŸ“„ weight [F32, (2048, 128256), 1002.0 MB]

Total Parameters: 1.5B | Selected: 83/83 | Scroll: 20 | Matches: 83



Total Parameters: 1.3B | Selected: 1/58 | Scroll: 0 | Matches: 58
